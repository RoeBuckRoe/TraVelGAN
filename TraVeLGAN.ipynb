{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraVeLGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1Dy_nn0FY_lW",
        "P_1-UuUOZcNM",
        "sgaThz_m9zTx",
        "xAgrvjAazmR7",
        "UFbtiu9rMqpi",
        "MyRhwd0JqJ4C",
        "lIQ9olngd5hk",
        "QqB9Q1x1gqtJ",
        "LAD-mWZ9gJZV",
        "UZop9zRijG0B",
        "pqUUFdFDjuZJ"
      ],
      "authorship_tag": "ABX9TyOjvIkpUCL4sumHUNN9iVTu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoeBuckRoe/TraVelGAN/blob/main/TraVeLGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dy_nn0FY_lW"
      },
      "source": [
        "### **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epeknGo6AkAo"
      },
      "source": [
        "from os import listdir\n",
        "from numpy import asarray\n",
        "from numpy import vstack\n",
        "from numpy import load\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import savez_compressed\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.optimizers import Adam\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.layers import Lambda\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Input\n",
        "from matplotlib import pyplot\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "init = tf.compat.v1.global_variables_initializer()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_1-UuUOZcNM"
      },
      "source": [
        "### **Uploading and processing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FGlqsORZRx0",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "aedd456f-b5cd-41c1-d13d-7d784cc37fcc"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0b3d57f2-3f57-4002-a457-7aab742b2a67\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0b3d57f2-3f57-4002-a457-7aab742b2a67\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving p2h.zip to p2h.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BByCMejQZS8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2204b2f9-6cd8-4956-a083-10b2a3510be2"
      },
      "source": [
        "!unzip -q 'p2h.zip' #unzip the uploaded files\n",
        "!ls #view the contents of the uploaded file"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p2h.zip  potr2hedc  potr2hedc_256.npz  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Y2BA1MZVMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d44665-1306-4aa2-c6f0-a2db41bc6987"
      },
      "source": [
        "# load all images in a directory into memory\n",
        "def load_images(path, size=(256,256,3)):\n",
        "\timg_list = list()\n",
        "\t# enumerate filenames in directory (all are images).\n",
        "\tfor filename in listdir(path):\n",
        "\t\t# load and resize the image\n",
        "\t\tpixels = load_img(path + filename, target_size=size)\n",
        "\t\t# convert to numpy array\n",
        "\t\tpixels = img_to_array(pixels)\n",
        "\t\timg= pixels\n",
        "\t\timg_list.append(img)\n",
        "\treturn [asarray(img_list)]\n",
        " \n",
        "# dataset path\n",
        "path = '/content/potr2hedc/potr/'\n",
        "# load dataset\n",
        "[src_images] = load_images(path)\n",
        "print('Loaded: ', src_images.shape)\n",
        "# save as compressed numpy array\n",
        "path = '/content/potr2hedc/hedc/'\n",
        "# load dataset\n",
        "[tar_images] = load_images(path)\n",
        "print('Loaded: ', tar_images.shape)\n",
        "filename = 'potr2hedc_256.npz'\n",
        "savez_compressed(filename, src_images, tar_images)\n",
        "print('Saved dataset: ', filename)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded:  (180, 256, 256, 3)\n",
            "Loaded:  (180, 256, 256, 3)\n",
            "Saved dataset:  potr2hedc_256.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgaThz_m9zTx"
      },
      "source": [
        "### **Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExFJrqtD944v"
      },
      "source": [
        "def discriminator(image_shape):\n",
        "  #init = RandomNormal(stddev=0.02) #wt initialization\n",
        "  in_input_im = Input(shape=image_shape) #input image input\n",
        "  in_gen_im = Input(shape=image_shape) # generated image input\n",
        "  merged = Concatenate()([in_input_im, in_gen_im]) # merge input images\n",
        "\n",
        "# Discriminator_model\n",
        "\n",
        "  # Conv32\n",
        "  d = Conv2D(32, (4,4), strides=(2,2), padding='same', \n",
        "             kernel_initializer=init)(merged)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "  # Conv64\n",
        "  d = Conv2D(64, (4,4), strides=(2,2), padding='same',\n",
        "             kernel_initializer=init)(d)\n",
        "  d = BatchNormalization()(d)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "  # Conv128\n",
        "  d = Conv2D(128, (4,4), strides=(2,2), padding='same',\n",
        "             kernel_initializer=init)(d)\n",
        "  d = BatchNormalization()(d)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "  # Conv256\n",
        "  d = Conv2D(256, (4,4), strides=(2,2), padding='same',\n",
        "             kernel_initializer=init)(d)\n",
        "  d = BatchNormalization()(d)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "  d = Conv2D(256, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "  d = BatchNormalization()(d)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "  # Output Layer\n",
        "  d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "  patch_out = Activation('sigmoid')(d)\n",
        " \n",
        "  model = Model([in_input_im, in_gen_im], patch_out) # define model\n",
        "  \n",
        "  #print(model.summary())\n",
        "\n",
        "  opt = Adam(learning_rate=0.0002, beta_1=0.5) # compile model\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAgrvjAazmR7"
      },
      "source": [
        "### **Generator** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWofh4koGyyv"
      },
      "source": [
        "**Encoder and Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CBxJT-RG2W7"
      },
      "source": [
        "def enc_block(layer_in, n_filters, batchnorm=True):\n",
        " \n",
        "  init = RandomNormal(stddev=0.02) # wt initialization\n",
        "\n",
        "  # add downsampling layer\n",
        "  g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same',\n",
        "  kernel_initializer=init)(layer_in)\n",
        "  # conditionally add batch normalization\n",
        "  if batchnorm:\n",
        "    g = BatchNormalization()(g, training=True)\n",
        "  # leaky relu activation\n",
        "  g = LeakyReLU(alpha=0.2)(g)\n",
        "  return g\n",
        "\n",
        "def dec_block(layer_in, skip_in, n_filters, dropout=True):\n",
        "\n",
        "  init = RandomNormal(stddev=0.02)  # wt initialization\n",
        "  # add upsampling layer\n",
        "  g = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same',\n",
        "  kernel_initializer=init)(layer_in)\n",
        "  # add batch normalization\n",
        "  g = BatchNormalization()(g, training=True)\n",
        "  # conditionally add dropout\n",
        "  if dropout:\n",
        "    g = Dropout(0.5)(g, training=True)\n",
        "  # merge with skip connection\n",
        "  g = Concatenate()([g, skip_in])\n",
        "  # relu activation\n",
        "  g = Activation('relu')(g)\n",
        "  return g"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT0MYDZ7HGeL"
      },
      "source": [
        "**Generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaDFouFFHT6k"
      },
      "source": [
        "def generator(image_shape=(256,256,3)):\n",
        "  init = RandomNormal(stddev=0.02) # wt initialization\n",
        "  in_image = Input(shape = image_shape) # image input\n",
        "\n",
        "# Generator_Model\n",
        "  \n",
        "  # encoder model\n",
        "  e1 = enc_block(in_image, 32, batchnorm=False)\n",
        "  e2 = enc_block(e1,64)  \n",
        "  e3 = enc_block(e2,128) \n",
        "  e4 = enc_block(e3,256)\n",
        "  e5 = enc_block(e4,256) \n",
        "  e6 = enc_block(e5,256) \n",
        "  e7 = enc_block(e6,256) \n",
        "\n",
        "  # bottlneck layer\n",
        "  b = Conv2D(512, (4,4), strides=(2,2), padding='same', \n",
        "             kernel_initializer=init)(e7)\n",
        "  b = Activation('relu')(b)\n",
        "\n",
        "  # decoder model\n",
        "  d1 = dec_block(b, e7, 256)\n",
        "  d2 = dec_block(d1, e6, 256)\n",
        "  d3 = dec_block(d2, e5, 256)\n",
        "  d4 = dec_block(d3, e4, 256, dropout=False)\n",
        "  d5 = dec_block(d4, e3, 128, dropout=False)\n",
        "  d6 = dec_block(d5, e2, 64, dropout=False)\n",
        "  d7 = dec_block(d6, e1, 32, dropout=False) \n",
        "\n",
        "  # output\n",
        "  g = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same',\n",
        "                      kernel_initializer=init)(d7)\n",
        "  out_image = Activation('tanh')(g)\n",
        "\n",
        "# define model\n",
        "  model = Model(in_image, out_image)\n",
        "\n",
        "  #print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFbtiu9rMqpi"
      },
      "source": [
        "### **Siamese**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGVNYHrG2sJU"
      },
      "source": [
        "def siamese(image_shape=(256,256,3)):\n",
        "\n",
        "# Siamese model\n",
        "  \n",
        "  # input\n",
        "  input_image = Input(image_shape)\n",
        "  # real samples\n",
        "  xi = Input(image_shape)\n",
        "  xj = Input(image_shape)\n",
        "  # generated samples\n",
        "  Geni = Input(image_shape)\n",
        "  Genj = Input(image_shape)\n",
        "\n",
        "  si = Conv2D(64, kernel_size=4, activation='relu')(xi)\n",
        "  si = MaxPooling2D()(si)\n",
        "  si = Conv2D(128, kernel_size=4, activation='relu')(si)\n",
        "  si = MaxPooling2D()(si)\n",
        "  si = Conv2D(128, kernel_size=4, activation='relu')(si)\n",
        "  si = MaxPooling2D()(si)\n",
        "  si = Conv2D(256, kernel_size=4, activation='relu')(si)\n",
        "  si = Flatten()(si)\n",
        "  si_out = Dense(4096, activation='sigmoid')(si)\n",
        "\n",
        "  sj = Conv2D(64, kernel_size=4, activation='relu')(xj)\n",
        "  sj = MaxPooling2D()(sj)\n",
        "  sj = Conv2D(128, kernel_size=4, activation='relu')(sj)\n",
        "  sj = MaxPooling2D()(sj)\n",
        "  sj = Conv2D(128, kernel_size=4, activation='relu')(sj)\n",
        "  sj = MaxPooling2D()(sj)\n",
        "  sj = Conv2D(256, kernel_size=4, activation='relu')(sj)\n",
        "  sj = Flatten()(sj)\n",
        "  sj_out = Dense(4096, activation='sigmoid')(sj)\n",
        "  \n",
        "  gi = Conv2D(64, kernel_size=4, activation='relu')(Geni)\n",
        "  gi = MaxPooling2D()(gi)\n",
        "  gi = Conv2D(128, kernel_size=4, activation='relu')(gi)\n",
        "  gi = MaxPooling2D()(gi)\n",
        "  gi = Conv2D(128, kernel_size=4, activation='relu')(gi)\n",
        "  gi = MaxPooling2D()(gi)\n",
        "  gi = Conv2D(256, kernel_size=4, activation='relu')(gi)\n",
        "  gi = Flatten()(gi)\n",
        "  gi_out = Dense(4096, activation='sigmoid')(gi)\n",
        "\n",
        "  gj = Conv2D(64, kernel_size=4, activation='relu')(Genj)\n",
        "  gj = MaxPooling2D()(gj)\n",
        "  gj = Conv2D(128, kernel_size=4, activation='relu')(gj)\n",
        "  gj = MaxPooling2D()(gj)\n",
        "  gj = Conv2D(128, kernel_size=4, activation='relu')(gj)\n",
        "  gj = MaxPooling2D()(gj)\n",
        "  gj = Conv2D(256, kernel_size=4, activation='relu')(gj)\n",
        "  gj = Flatten()(gj)\n",
        "  gj_out = Dense(4096, activation='sigmoid')(gj)\n",
        "\n",
        "  L_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "\n",
        "  L1_distance = L_layer([si_out, gi_out])\n",
        "  L2_distance = L_layer([sj_out, gj_out])\n",
        "\n",
        "  predn1 = Dense(1,activation='sigmoid',bias_initializer='zeros')(L1_distance)\n",
        "  predn2 = Dense(1,activation='sigmoid',bias_initializer='zeros')(L2_distance)\n",
        "  \n",
        "  model = Model(inputs=[xi, xj, Geni, Genj], outputs = [predn1, predn2])\n",
        "\n",
        "  optimizer = Adam(learning_rate = 0.00006)\n",
        "  model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
        "\n",
        "  #print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyRhwd0JqJ4C"
      },
      "source": [
        "### GAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSUlig1YqVdw"
      },
      "source": [
        "def Tvel_gan(g_model, d_model, s_model, image_shape):\n",
        "\n",
        "  #d_model.trainable = False\n",
        "\n",
        "  # source images\n",
        "  in_xi = Input(image_shape)\n",
        "  in_xj = Input(image_shape)\n",
        "\n",
        "  # generating outputs for two source images\n",
        "  gen_outi = g_model(in_xi)\n",
        "  gen_outj = g_model(in_xj)\n",
        "\n",
        "  # discriminator output\n",
        "  dis_out = d_model([in_xi, gen_outi])\n",
        "\n",
        "  # siamese network output\n",
        "  sia_out = s_model([in_xi, in_xj, gen_outi, gen_outj])\n",
        "\n",
        "  # define model\n",
        "  model = Model([in_xi, in_xj], [dis_out, gen_outi, sia_out])\n",
        "\n",
        "  #print(model.summary())\n",
        "\n",
        "  # compile model\n",
        "  opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "  model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
        "  return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIQ9olngd5hk"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV65LAxmd9gt"
      },
      "source": [
        "def load_real_samples(filename):\n",
        "  # load the compressed arrays\n",
        "  data = load(filename)\n",
        "  # unpack the arrays\n",
        "  X1, X2 = data['arr_0'], data['arr_1']\n",
        "  # scale from [0,255] to [-1,1]\n",
        "  X1 = (X1 - 127.5) / 127.5\n",
        "  X2 = (X2 - 127.5) / 127.5\n",
        "  return [X1, X2]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqB9Q1x1gqtJ"
      },
      "source": [
        "### Real Sample Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_i27V-sgycJ"
      },
      "source": [
        "def generate_real_samples(dataset, n_samples, patch_shape):\n",
        "  # unpack dataset\n",
        "  trainA, trainB = dataset\n",
        "  # choose a random instance\n",
        "  xi = randint(0, trainA.shape[0], n_samples)\n",
        "  xj = randint(0, trainA.shape[0], n_samples)\n",
        "  # retrieve selected images\n",
        "  xi1, xi2 = trainA[xi], trainB[xi]\n",
        "  xj = trainA[xj]\n",
        "  y = ones((n_samples, patch_shape, patch_shape, 1))\n",
        "  return [xi1, xi2], xj, y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAD-mWZ9gJZV"
      },
      "source": [
        "### Fake Sample Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxshMTMqgPeR"
      },
      "source": [
        "def generate_fake_samples(g_model, samples, patch_shape):\n",
        "  # generate fake instance\n",
        "  X = g_model.predict(samples)\n",
        "  # create 'fake' class labels (0)\n",
        "  y = zeros((len(X), patch_shape, patch_shape, 1))\n",
        "  return X, y"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZop9zRijG0B"
      },
      "source": [
        "### G_Model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjGm4iLLjpmJ"
      },
      "source": [
        "def summarize_performance(step, g_model, dataset, n_samples=3):\n",
        "  # select a sample of input images\n",
        "  [X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n",
        "  # generate a batch of fake samples\n",
        "  X_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
        "  # scale all pixels from [-1,1] to [0,1]\n",
        "  X_realA = (X_realA + 1) / 2.0\n",
        "  X_realB = (X_realB + 1) / 2.0\n",
        "  X_fakeB = (X_fakeB + 1) / 2.0\n",
        "  # plot real source images\n",
        "  for i in range(n_samples):\n",
        "    pyplot.subplot(3, n_samples, 1 + i)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(X_realA[i])\n",
        "    # plot generated target image\n",
        "  for i in range(n_samples):\n",
        "    pyplot.subplot(3, n_samples, 1 + n_samples + i)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(X_fakeB[i])\n",
        "    # plot real target image\n",
        "  for i in range(n_samples):\n",
        "    pyplot.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(X_realB[i])\n",
        "  # save plot to file\n",
        "  filename1 = 'plot_%06d.png' % (step+1)\n",
        "  pyplot.savefig(filename1)\n",
        "  pyplot.close()\n",
        "  # save the generator model\n",
        "  filename2 = 'model_%06d.h5' % (step+1)\n",
        "  g_model.save(filename2)\n",
        "  print('>Saved: %s and %s' % (filename1, filename2))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqUUFdFDjuZJ"
      },
      "source": [
        "### Model Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmgNcE_9j6Fp"
      },
      "source": [
        "def train(d_model, g_model, s_model, gan_model, dataset, n_epochs=250, n_batch=1):\n",
        "  # determine the output square shape of the discriminator\n",
        "  n_patch = d_model.output_shape[1]\n",
        "  # unpack dataset\n",
        "  trainA, trainB = dataset\n",
        "  print(\"unpacked\")\n",
        "  # calculate the number of batches per training epoch\n",
        "  bat_per_epo = int(len(trainA) / n_batch)\n",
        "  print(\"bat_per_epo calculated\")\n",
        "  # calculate the number of training iterations\n",
        "  n_steps = bat_per_epo * n_epochs\n",
        "  print(\"training iteration calculated\")\n",
        "  # manually enumerate epochs\n",
        "  for i in range(n_steps):\n",
        "    # select a batch of real samples\n",
        "    [X_realA, X_realB], X_j, y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
        "    print(\"batch of real samples\")\n",
        "    # generate a batch of fake samples\n",
        "    X_geni, y_geni = generate_fake_samples(g_model, X_realA, n_patch)\n",
        "    X_genj, y_genj = generate_fake_samples(g_model, X_j, n_patch)\n",
        "    print(\"fake sample generated\")\n",
        "    # update discriminator for real samples\n",
        "    d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
        "    print(\"d_loss1\")\n",
        "    # update discriminator for generated samples\n",
        "    d_loss2 = d_model.train_on_batch([X_realA, X_geni], y_geni)\n",
        "    print(\"d_loss2\")\n",
        "    # update the siamese network\n",
        "    s_loss = s_model.train_on_batch([X_realA, X_j, X_geni, X_genj])\n",
        "    print(\"s_loss\")\n",
        "    # update the generator\n",
        "    g_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
        "    print(\"g_loss\")\n",
        "    # summarize performance\n",
        "    print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
        "    # summarize model performance\n",
        "    if (i+1) % (bat_per_epo * 100) == 0:\n",
        "      summarize_performance(i, g_model, dataset)\n",
        "      summarize_performance(i, s_model, dataset) "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFg2NdD5xuJY"
      },
      "source": [
        "### Main Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yoq8HENVxyIf"
      },
      "source": [
        "dataset = load_real_samples('potr2hedc_256.npz')\n",
        "print('Loaded', dataset[0].shape, dataset[1].shape)\n",
        "# define input shape based on the loaded dataset\n",
        "image_shape = dataset[0].shape[1:]\n",
        "# define the models\n",
        "d_model = discriminator(image_shape)\n",
        "print(\"D_MODEL\")\n",
        "g_model = generator(image_shape)\n",
        "print(\"G_MODEL\")\n",
        "s_model = siamese(image_shape)\n",
        "print(\"S_MODEL\")\n",
        "# define the composite model\n",
        "gan_model = Tvel_gan(g_model, d_model, s_model, image_shape)\n",
        "print(\"GAN_MODEL\")\n",
        "# train model\n",
        "train(d_model, g_model, s_model, gan_model, dataset)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}